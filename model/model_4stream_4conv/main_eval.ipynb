{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from epinet_fun.func_generate_traindata import generate_traindata_for_train\n",
    "from epinet_fun.func_generate_traindata import data_augmentation_for_train\n",
    "from epinet_fun.func_generate_traindata import generate_traindata512\n",
    "from epinet_fun.func_makeinput import make_multiinput\n",
    "from epinet_fun.func_pfm import read_pfm\n",
    "from epinet_fun.func_savedata import display_current_output\n",
    "from epinet_fun.util import load_LFdata\n",
    "\n",
    "from network.model_for_eval import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "import configparser\n",
    "import json\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "#from epinet_fun.func_middle_output import middle_layer_output\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inifile = configparser.ConfigParser()\n",
    "inifile.read('./config.ini', 'UTF-8')\n",
    "\n",
    "train_dataset_list = json.loads(inifile.get('dataset_list','train_dataset_list'))\n",
    "test_dataset_list = json.loads(inifile.get('dataset_list','test_dataset_list'))\n",
    "\n",
    "dataset_path = inifile.get('PATH','dataset_path')\n",
    "boolmask_img4_path = inifile.get('PATH','boolmask_img4')\n",
    "boolmask_img6_path = inifile.get('PATH','boolmask_img6')\n",
    "boolmask_img15_path = inifile.get('PATH','boolmask_img15')\n",
    "\n",
    "image_width = int(inifile.get('model_1371','image_width'))\n",
    "image_height = int(inifile.get('model_1371','image_height'))\n",
    "\n",
    "batch_size_training = int(inifile.get('training_general','batch_size_training'))\n",
    "batch_size_validation = int(inifile.get('training_general','batch_size_validation'))\n",
    "batch_num_in_1epoch_for_training = int(inifile.get('training_general','batch_num_in_1epoch_for_training'))\n",
    "training_img_size = int(inifile.get('training_general','training_img_size'))\n",
    "validation_img_size = int(inifile.get('training_general','validation_img_size'))\n",
    "\n",
    "learning_rate = float(inifile.get('training_general','learning_rate'))\n",
    "validation_frequency = int(inifile.get('training_general','validation_frequency'))\n",
    "save_model_frequency = int(inifile.get('training_general','save_model_frequency'))\n",
    "\n",
    "input_ch = int(inifile.get('training_general','input_ch'))\n",
    "filter_num = int(inifile.get('training_general','filter_num'))\n",
    "stream_num = int(inifile.get('training_general','stream_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    normalized_image = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    image_uint8 = np.uint8(normalized_image * 255)\n",
    "\n",
    "    concatenated_images = np.hstack(image_uint8)\n",
    "    imageio.imsave(save_path, np.squeeze(concatenated_images))\n",
    "\n",
    "def save_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    imageio.imsave(save_path, np.squeeze(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_optimizer(model, optimizer, save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    state = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_current_output(train_output, traindata_label, save_path):\n",
    "    '''\n",
    "        display current results from EPINET\n",
    "        and save results in /current_output\n",
    "    '''\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    sz=len(traindata_label)\n",
    "    train_output=np.squeeze(train_output)\n",
    "    if(len(traindata_label.shape)>3 and traindata_label.shape[-1]==9): # traindata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15,4,4]\n",
    "    else: # valdata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15]\n",
    "\n",
    "    train_output482=train_output[:,15-pad1_half:482+15-pad1_half,15-pad1_half:482+15-pad1_half]\n",
    "\n",
    "    train_diff=np.abs(train_output482-train_label482)\n",
    "    train_bp=(train_diff>=0.07)\n",
    "    condition = train_bp > 0\n",
    "\n",
    "    bp_img=np.zeros_like(train_bp)\n",
    "    bp_img[condition] = 1\n",
    "    bp_img[~condition] = 0\n",
    "    \n",
    "\n",
    "    train_output482_all=np.zeros((3*482,sz*482),np.uint8)\n",
    "    train_output482_all[0:482,:]=np.uint8(25*np.reshape(np.transpose(train_label482,(1,0,2)),(482,sz*482))+100)\n",
    "    train_output482_all[482:2*482,:]=np.uint8(25*np.reshape(np.transpose(train_output482,(1,0,2)),(482,sz*482))+100)\n",
    "    train_output482_all[2*482:3*482,:]=np.uint8(25*np.reshape(np.transpose(bp_img,(1,0,2)),(482,sz*482))+100)\n",
    "\n",
    "    imageio.imsave(save_path, np.squeeze(train_output482_all))\n",
    "\n",
    "    return train_diff, train_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,traindata_all, traindata_label, input_size,label_size,batch_size,Setting02_AngualrViews,\n",
    "                                                boolmask_img4,boolmask_img6,boolmask_img15, batch_num_in_1epoch, mode):\n",
    "        self.traindata_all = traindata_all\n",
    "        self.traindata_label = traindata_label\n",
    "        self.input_size = input_size\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.Setting02_AngualrViews = Setting02_AngualrViews\n",
    "        self.boolmask_img4 = boolmask_img4\n",
    "        self.boolmask_img6 = boolmask_img6\n",
    "        self.boolmask_img15 = boolmask_img15\n",
    "        self.batch_num_in_1epoch = batch_num_in_1epoch\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (traindata_batch_90d, traindata_batch_0d,traindata_batch_45d, traindata_batch_m45d,\n",
    "        traindata_label_batchNxN)= generate_traindata_for_train(self.traindata_all,self.traindata_label,\n",
    "                                                                self.input_size,self.label_size,1,\n",
    "                                                                self.Setting02_AngualrViews,\n",
    "                                                                self.boolmask_img4,self.boolmask_img6,self.boolmask_img15,self.mode)\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (1, 25, 25, 9) nd.array\n",
    "         traindata_label_batchNxN : (1, 3, 3) nd.array\n",
    "        \"\"\"\n",
    "\n",
    "        (traindata_batch_90d_aug, traindata_batch_0d_aug,traindata_batch_45d_aug,traindata_batch_m45d_aug,\n",
    "        traindata_label_batchNxN_aug) =  data_augmentation_for_train(traindata_batch_90d,traindata_batch_0d,\n",
    "                                                                traindata_batch_45d,traindata_batch_m45d,\n",
    "                                                                traindata_label_batchNxN, 1)\n",
    "\n",
    "        traindata_batch_90d = torch.from_numpy(traindata_batch_90d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_0d = torch.from_numpy(traindata_batch_0d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_45d = torch.from_numpy(traindata_batch_45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_m45d = torch.from_numpy(traindata_batch_m45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_label_batchNxN = torch.from_numpy(traindata_label_batchNxN_aug).squeeze(0).to(torch.float32)\n",
    "\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (9, 25, 25) tensor\n",
    "         traindata_label_batchNxN : (3, 3) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return traindata_batch_90d, traindata_batch_0d, traindata_batch_45d, traindata_batch_m45d, traindata_label_batchNxN\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size * self.batch_num_in_1epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional/antinous\n",
      "additional/boardgames\n",
      "additional/dishes\n",
      "additional/greek\n",
      "additional/kitchen\n",
      "additional/medieval2\n",
      "additional/museum\n",
      "additional/pens\n",
      "additional/pillows\n",
      "additional/platonic\n",
      "additional/rosemary\n",
      "additional/table\n",
      "additional/tomb\n",
      "additional/tower\n",
      "additional/town\n",
      "additional/vinyl\n",
      "stratified/backgammon\n",
      "stratified/dots\n",
      "stratified/pyramids\n",
      "stratified/stripes\n",
      "training/boxes\n",
      "training/cotton\n",
      "training/dino\n",
      "training/sideboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\\n traindata_label : (16, 512, 512) nd.array\\n testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\\n testdata_label : (8, 512, 512)   nd.array\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_all,traindata_label=load_LFdata(train_dataset_list)\n",
    "testdata_all,testdata_label=load_LFdata(test_dataset_list)\n",
    "\"\"\"\n",
    " traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\n",
    " traindata_label : (16, 512, 512) nd.array\n",
    " testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\n",
    " testdata_label : (8, 512, 512)   nd.array\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=23+2         # Input size should be greater than or equal to 23\n",
    "label_size=input_size-22 # Since label_size should be greater than or equal to 1\n",
    "Setting02_AngualrViews = np.array([0,1,2,3,4,5,6,7,8])  # number of views ( 0~8 for 9x9 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make training tensor for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset tensor size : torch.Size([512, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "training_full_90d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_0d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_M45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "\n",
    "for batch, image_path in enumerate(train_dataset_list):\n",
    "    \n",
    "    image_path = os.path.join(dataset_path, image_path)\n",
    "    (train_90d_np, train_0d_np, train_45d_np, train_M45d_np) = make_multiinput(image_path,\n",
    "                                                                              training_img_size,\n",
    "                                                                              training_img_size,\n",
    "                                                                              Setting02_AngualrViews)\n",
    "    train_90d = torch.from_numpy(np.squeeze(train_90d_np))\n",
    "    train_0d = torch.from_numpy(np.squeeze(train_0d_np))\n",
    "    train_45d = torch.from_numpy(np.squeeze(train_45d_np))\n",
    "    train_M45d = torch.from_numpy(np.squeeze(train_M45d_np))\n",
    "\n",
    "    training_full_90d[batch, :, :, :] = train_90d\n",
    "    training_full_0d[batch, :, :, :] = train_0d\n",
    "    training_full_45d[batch, :, :, :] = train_45d\n",
    "    training_full_M45d[batch, :, :, :] = train_M45d\n",
    "\n",
    "training_full_90d = training_full_90d.permute(0, 3, 1, 2)\n",
    "training_full_0d = training_full_0d.permute(0, 3, 1, 2)\n",
    "training_full_45d = training_full_45d.permute(0, 3, 1, 2)\n",
    "training_full_M45d = training_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"training dataset tensor size : {training_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset tensor size : torch.Size([8, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "validation_full_90d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_0d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_M45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "\n",
    "for batch, image_path in enumerate(test_dataset_list):\n",
    "\n",
    "    image_path = os.path.join(dataset_path,image_path)\n",
    "    (val_90d_np , val_0d_np, val_45d_np, val_M45d_np)=make_multiinput(image_path,\n",
    "                                                            validation_img_size,\n",
    "                                                            validation_img_size,\n",
    "                                                            Setting02_AngualrViews)\n",
    "    val_90d = torch.from_numpy(np.squeeze(val_90d_np))\n",
    "    val_0d = torch.from_numpy(np.squeeze(val_0d_np))\n",
    "    val_45d = torch.from_numpy(np.squeeze(val_45d_np))\n",
    "    val_M45d = torch.from_numpy(np.squeeze(val_M45d_np))\n",
    "\n",
    "    validation_full_90d[batch, :, :, :] = val_90d\n",
    "    validation_full_0d[batch, :, :, :] = val_0d\n",
    "    validation_full_45d[batch, :, :, :] = val_45d\n",
    "    validation_full_M45d[batch, :, :, :] = val_M45d\n",
    "\n",
    "validation_full_90d = validation_full_90d.permute(0, 3, 1, 2)\n",
    "validation_full_0d = validation_full_0d.permute(0, 3, 1, 2)\n",
    "validation_full_45d = validation_full_45d.permute(0, 3, 1, 2)\n",
    "validation_full_M45d = validation_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"validation dataset tensor size : {validation_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup boolmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolmask_img4_path : ../../hci_dataset/additional_invalid_area/kitchen/input_Cam040_invalid_ver2.png\n"
     ]
    }
   ],
   "source": [
    "print(f\"boolmask_img4_path : {boolmask_img4_path}\")\n",
    "boolmask_img4 = np.array(Image.open(boolmask_img4_path))\n",
    "boolmask_img6 = np.array(Image.open(boolmask_img6_path))\n",
    "boolmask_img15 = np.array(Image.open(boolmask_img15_path))\n",
    "\n",
    "boolmask_img4  = 1.0*boolmask_img4[:,:,3]>0\n",
    "boolmask_img6  = 1.0*boolmask_img6[:,:,3]>0\n",
    "boolmask_img15 = 1.0*boolmask_img15[:,:,3]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize loss txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(traindata_all, traindata_label, input_size,label_size,batch_size_training,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,batch_num_in_1epoch_for_training,mode=\"training\")\n",
    "test_dataset = CustomDataset(testdata_all, testdata_label, input_size,label_size,batch_size_validation,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,1,mode=\"validation\")\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size_training,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size_validation,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EPINET(input_ch = input_ch, filter_num = filter_num, stream_num =stream_num).to(\"cuda\")\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in range(8):\n",
    "\n",
    "        x_0d = validation_full_0d[batch,:,:,:].clone().to(\"cuda\").unsqueeze(0)\n",
    "        x_90d = validation_full_90d[batch,:,:,:].clone().to(\"cuda\").unsqueeze(0)\n",
    "        x_45d = validation_full_45d[batch,:,:,:].clone().to(\"cuda\").unsqueeze(0)\n",
    "        x_m45d = validation_full_M45d[batch,:,:,:].clone().to(\"cuda\").unsqueeze(0)\n",
    "\n",
    "        outputs = net(x_0d, x_90d, x_45d, x_m45d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
