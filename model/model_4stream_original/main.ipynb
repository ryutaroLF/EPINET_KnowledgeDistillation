{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from epinet_fun.func_generate_traindata import generate_traindata_for_train\n",
    "from epinet_fun.func_generate_traindata import data_augmentation_for_train\n",
    "from epinet_fun.func_generate_traindata import generate_traindata512\n",
    "from epinet_fun.func_makeinput import make_multiinput\n",
    "from epinet_fun.func_pfm import read_pfm\n",
    "from epinet_fun.func_savedata import display_current_output\n",
    "from epinet_fun.util import load_LFdata\n",
    "\n",
    "from network.model import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "import configparser\n",
    "import json\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "#from epinet_fun.func_middle_output import middle_layer_output\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inifile = configparser.ConfigParser()\n",
    "inifile.read('./config.ini', 'UTF-8')\n",
    "\n",
    "train_dataset_list = json.loads(inifile.get('dataset_list','train_dataset_list'))\n",
    "test_dataset_list = json.loads(inifile.get('dataset_list','test_dataset_list'))\n",
    "\n",
    "dataset_path = inifile.get('PATH','dataset_path')\n",
    "boolmask_img4_path = inifile.get('PATH','boolmask_img4')\n",
    "boolmask_img6_path = inifile.get('PATH','boolmask_img6')\n",
    "boolmask_img15_path = inifile.get('PATH','boolmask_img15')\n",
    "\n",
    "image_width = int(inifile.get('model_1371','image_width'))\n",
    "image_height = int(inifile.get('model_1371','image_height'))\n",
    "\n",
    "batch_size_training = int(inifile.get('training_general','batch_size_training'))\n",
    "batch_size_validation = int(inifile.get('training_general','batch_size_validation'))\n",
    "batch_num_in_1epoch_for_training = int(inifile.get('training_general','batch_num_in_1epoch_for_training'))\n",
    "training_img_size = int(inifile.get('training_general','training_img_size'))\n",
    "validation_img_size = int(inifile.get('training_general','validation_img_size'))\n",
    "\n",
    "learning_rate = float(inifile.get('training_general','learning_rate'))\n",
    "LR_scheduler_change_point_iteration = int(inifile.get('training_general','LR_scheduler_change_point_iteration'))\n",
    "validation_frequency = int(inifile.get('training_general','validation_frequency'))\n",
    "save_model_frequency = int(inifile.get('training_general','save_model_frequency'))\n",
    "\n",
    "input_ch = int(inifile.get('training_general','input_ch'))\n",
    "filter_num = int(inifile.get('training_general','filter_num'))\n",
    "stream_num = int(inifile.get('training_general','stream_num'))\n",
    "input_size = int(inifile.get('training_general','input_size'))\n",
    "label_size = int(inifile.get('training_general','label_size'))\n",
    "\n",
    "seed = int(inifile.get('training_general','seed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    normalized_image = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    image_uint8 = np.uint8(normalized_image * 255)\n",
    "\n",
    "    concatenated_images = np.hstack(image_uint8)\n",
    "    imageio.imsave(save_path, np.squeeze(concatenated_images))\n",
    "\n",
    "def save_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    imageio.imsave(save_path, np.squeeze(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_optimizer(model, optimizer, save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    state = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_current_output(train_output, traindata_label, save_path,save_img_flag):\n",
    "    '''\n",
    "        display current results from EPINET\n",
    "        and save results in /current_output\n",
    "    '''\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    sz=len(traindata_label)\n",
    "    train_output=np.squeeze(train_output)\n",
    "    if(len(traindata_label.shape)>3 and traindata_label.shape[-1]==9): # traindata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15,4,4]\n",
    "    else: # valdata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15]\n",
    "\n",
    "    train_output482=train_output[:,15-pad1_half:482+15-pad1_half,15-pad1_half:482+15-pad1_half]\n",
    "\n",
    "    train_diff=np.abs(train_output482-train_label482)\n",
    "    train_bp=(train_diff>=0.07)\n",
    "\n",
    "    if save_img_flag:\n",
    "        condition = train_bp > 0\n",
    "\n",
    "        bp_img=np.zeros_like(train_bp)\n",
    "        bp_img[condition] = 1\n",
    "        bp_img[~condition] = 0\n",
    "        \n",
    "\n",
    "        train_output482_all=np.zeros((3*482,sz*482),np.uint8)\n",
    "        train_output482_all[0:482,:]=np.uint8(25*np.reshape(np.transpose(train_label482,(1,0,2)),(482,sz*482))+100)\n",
    "        train_output482_all[482:2*482,:]=np.uint8(25*np.reshape(np.transpose(train_output482,(1,0,2)),(482,sz*482))+100)\n",
    "        train_output482_all[2*482:3*482,:]=np.uint8(25*np.reshape(np.transpose(bp_img,(1,0,2)),(482,sz*482))+100)\n",
    "\n",
    "        imageio.imsave(save_path, np.squeeze(train_output482_all))\n",
    "\n",
    "    return train_diff, train_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,traindata_all, traindata_label, input_size,label_size,batch_size,Setting02_AngualrViews,\n",
    "                                                boolmask_img4,boolmask_img6,boolmask_img15, batch_num_in_1epoch, mode):\n",
    "        self.traindata_all = traindata_all\n",
    "        self.traindata_label = traindata_label\n",
    "        self.input_size = input_size\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.Setting02_AngualrViews = Setting02_AngualrViews\n",
    "        self.boolmask_img4 = boolmask_img4\n",
    "        self.boolmask_img6 = boolmask_img6\n",
    "        self.boolmask_img15 = boolmask_img15\n",
    "        self.batch_num_in_1epoch = batch_num_in_1epoch\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (traindata_batch_90d, traindata_batch_0d,traindata_batch_45d, traindata_batch_m45d,\n",
    "        traindata_label_batchNxN)= generate_traindata_for_train(self.traindata_all,self.traindata_label,\n",
    "                                                                self.input_size,self.label_size,1,\n",
    "                                                                self.Setting02_AngualrViews,\n",
    "                                                                self.boolmask_img4,self.boolmask_img6,self.boolmask_img15,self.mode)\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (1, 25, 25, 9) nd.array\n",
    "         traindata_label_batchNxN : (1, 3, 3) nd.array\n",
    "        \"\"\"\n",
    "\n",
    "        (traindata_batch_90d_aug, traindata_batch_0d_aug,traindata_batch_45d_aug,traindata_batch_m45d_aug,\n",
    "        traindata_label_batchNxN_aug) =  data_augmentation_for_train(traindata_batch_90d,traindata_batch_0d,\n",
    "                                                                traindata_batch_45d,traindata_batch_m45d,\n",
    "                                                                traindata_label_batchNxN, 1)\n",
    "\n",
    "        traindata_batch_90d = torch.from_numpy(traindata_batch_90d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_0d = torch.from_numpy(traindata_batch_0d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_45d = torch.from_numpy(traindata_batch_45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_m45d = torch.from_numpy(traindata_batch_m45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_label_batchNxN = torch.from_numpy(traindata_label_batchNxN_aug).squeeze(0).to(torch.float32)\n",
    "\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (9, 25, 25) tensor\n",
    "         traindata_label_batchNxN : (3, 3) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return traindata_batch_90d, traindata_batch_0d, traindata_batch_45d, traindata_batch_m45d, traindata_label_batchNxN\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size * self.batch_num_in_1epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional/antinous\n",
      "additional/boardgames\n",
      "additional/dishes\n",
      "additional/greek\n",
      "additional/kitchen\n",
      "additional/medieval2\n",
      "additional/museum\n",
      "additional/pens\n",
      "additional/pillows\n",
      "additional/platonic\n",
      "additional/rosemary\n",
      "additional/table\n",
      "additional/tomb\n",
      "additional/tower\n",
      "additional/town\n",
      "additional/vinyl\n",
      "stratified/backgammon\n",
      "stratified/dots\n",
      "stratified/pyramids\n",
      "stratified/stripes\n",
      "training/boxes\n",
      "training/cotton\n",
      "training/dino\n",
      "training/sideboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\\n traindata_label : (16, 512, 512) nd.array\\n testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\\n testdata_label : (8, 512, 512)   nd.array\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_all,traindata_label=load_LFdata(train_dataset_list)\n",
    "testdata_all,testdata_label=load_LFdata(test_dataset_list)\n",
    "\"\"\"\n",
    " traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\n",
    " traindata_label : (16, 512, 512) nd.array\n",
    " testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\n",
    " testdata_label : (8, 512, 512)   nd.array\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=input_size        # Input size should be greater than or equal to 23\n",
    "label_size=label_size # Since label_size should be greater than or equal to 1\n",
    "Setting02_AngualrViews = np.array([0,1,2,3,4,5,6,7,8])  # number of views ( 0~8 for 9x9 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make training tensor for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset tensor size : torch.Size([16, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "training_full_90d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_0d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_M45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "\n",
    "for batch, image_path in enumerate(train_dataset_list):\n",
    "    \n",
    "    image_path = os.path.join(dataset_path, image_path)\n",
    "    (train_90d_np, train_0d_np, train_45d_np, train_M45d_np) = make_multiinput(image_path,\n",
    "                                                                              training_img_size,\n",
    "                                                                              training_img_size,\n",
    "                                                                              Setting02_AngualrViews)\n",
    "    train_90d = torch.from_numpy(np.squeeze(train_90d_np))\n",
    "    train_0d = torch.from_numpy(np.squeeze(train_0d_np))\n",
    "    train_45d = torch.from_numpy(np.squeeze(train_45d_np))\n",
    "    train_M45d = torch.from_numpy(np.squeeze(train_M45d_np))\n",
    "\n",
    "    training_full_90d[batch, :, :, :] = train_90d\n",
    "    training_full_0d[batch, :, :, :] = train_0d\n",
    "    training_full_45d[batch, :, :, :] = train_45d\n",
    "    training_full_M45d[batch, :, :, :] = train_M45d\n",
    "\n",
    "training_full_90d = training_full_90d.permute(0, 3, 1, 2)\n",
    "training_full_0d = training_full_0d.permute(0, 3, 1, 2)\n",
    "training_full_45d = training_full_45d.permute(0, 3, 1, 2)\n",
    "training_full_M45d = training_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"training dataset tensor size : {training_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset tensor size : torch.Size([8, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "validation_full_90d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_0d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_M45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "\n",
    "for batch, image_path in enumerate(test_dataset_list):\n",
    "\n",
    "    image_path = os.path.join(dataset_path,image_path)\n",
    "    (val_90d_np , val_0d_np, val_45d_np, val_M45d_np)=make_multiinput(image_path,\n",
    "                                                            validation_img_size,\n",
    "                                                            validation_img_size,\n",
    "                                                            Setting02_AngualrViews)\n",
    "    val_90d = torch.from_numpy(np.squeeze(val_90d_np))\n",
    "    val_0d = torch.from_numpy(np.squeeze(val_0d_np))\n",
    "    val_45d = torch.from_numpy(np.squeeze(val_45d_np))\n",
    "    val_M45d = torch.from_numpy(np.squeeze(val_M45d_np))\n",
    "\n",
    "    validation_full_90d[batch, :, :, :] = val_90d\n",
    "    validation_full_0d[batch, :, :, :] = val_0d\n",
    "    validation_full_45d[batch, :, :, :] = val_45d\n",
    "    validation_full_M45d[batch, :, :, :] = val_M45d\n",
    "\n",
    "validation_full_90d = validation_full_90d.permute(0, 3, 1, 2)\n",
    "validation_full_0d = validation_full_0d.permute(0, 3, 1, 2)\n",
    "validation_full_45d = validation_full_45d.permute(0, 3, 1, 2)\n",
    "validation_full_M45d = validation_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"validation dataset tensor size : {validation_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup boolmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolmask_img4_path : ../../hci_dataset/additional_invalid_area/kitchen/input_Cam040_invalid_ver2.png\n"
     ]
    }
   ],
   "source": [
    "print(f\"boolmask_img4_path : {boolmask_img4_path}\")\n",
    "boolmask_img4 = np.array(Image.open(boolmask_img4_path))\n",
    "boolmask_img6 = np.array(Image.open(boolmask_img6_path))\n",
    "boolmask_img15 = np.array(Image.open(boolmask_img15_path))\n",
    "\n",
    "boolmask_img4  = 1.0*boolmask_img4[:,:,3]>0\n",
    "boolmask_img6  = 1.0*boolmask_img6[:,:,3]>0\n",
    "boolmask_img15 = 1.0*boolmask_img15[:,:,3]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize loss txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"loss\"):\n",
    "    os.makedirs(\"loss\")\n",
    "\n",
    "with open(\"./loss/loss_training.txt\", \"w\") as file:\n",
    "    file.write(f\"==================\\n\")\n",
    "with open(\"./loss/loss_validation.txt\", \"w\") as file:\n",
    "    file.write(f\"==================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(traindata_all, traindata_label, input_size,label_size,batch_size_training,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,batch_num_in_1epoch_for_training,mode=\"training\")\n",
    "test_dataset = CustomDataset(testdata_all, testdata_label, input_size,label_size,batch_size_validation,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,1,mode=\"validation\")\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size_training,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size_validation,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EPINET(input_ch = input_ch, filter_num = filter_num, stream_num =stream_num).to(\"cuda\")\n",
    "def mae_loss(output, target):\n",
    "    return F.l1_loss(output, target, reduction='mean')\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=learning_rate)\n",
    "\n",
    "#LR_change_epoch = LR_scheduler_change_point_iteration / batch_num_in_1epoch_for_training\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[LR_change_epoch], gamma=0.1)\n",
    "#print(f\"LR will change at : {LR_change_epoch} epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_BadPixel = 100\n",
    "best_MSE = 100\n",
    "best_validation_loss = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "epoch : 0\n",
      "Validating\n",
      "epoch : 1\n",
      "Validating\n",
      "epoch : 2\n",
      "epoch : 3\n",
      "epoch : 4\n",
      "Validating\n",
      "epoch : 5\n",
      "epoch : 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#===============================#\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#           Training            #\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#===============================#\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nBatch, (x_90d, x_0d, x_45d, x_m45d, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#print(nBatch)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     loss_division_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m     train_loss_Batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryuta\\anaconda3\\envs\\EPINET\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ryuta\\anaconda3\\envs\\EPINET\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ryuta\\anaconda3\\envs\\EPINET\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ryuta\\anaconda3\\envs\\EPINET\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     17\u001b[0m     (traindata_batch_90d, traindata_batch_0d,traindata_batch_45d, traindata_batch_m45d,\n\u001b[1;32m---> 18\u001b[0m     traindata_label_batchNxN)\u001b[38;5;241m=\u001b[39m generate_traindata_for_train(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraindata_all,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraindata_label,\n\u001b[0;32m     19\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_size,\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     20\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSetting02_AngualrViews,\n\u001b[0;32m     21\u001b[0m                                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboolmask_img4,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboolmask_img6,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboolmask_img15,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m     traindata_batch_0d : (1, 25, 25, 9) nd.array\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m     traindata_label_batchNxN : (1, 3, 3) nd.array\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     (traindata_batch_90d_aug, traindata_batch_0d_aug,traindata_batch_45d_aug,traindata_batch_m45d_aug,\n\u001b[0;32m     28\u001b[0m     traindata_label_batchNxN_aug) \u001b[38;5;241m=\u001b[39m  data_augmentation_for_train(traindata_batch_90d,traindata_batch_0d,\n\u001b[0;32m     29\u001b[0m                                                             traindata_batch_45d,traindata_batch_m45d,\n\u001b[0;32m     30\u001b[0m                                                             traindata_label_batchNxN, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ryuta\\EPINET_new\\EPINET_torch\\model_4stream_original\\epinet_fun\\func_generate_traindata.py:168\u001b[0m, in \u001b[0;36mgenerate_traindata_for_train\u001b[1;34m(traindata_all, traindata_label, input_size, label_size, batch_size, Setting02_AngualrViews, boolmask_img4, boolmask_img6, boolmask_img15, mode)\u001b[0m\n\u001b[0;32m    161\u001b[0m traindata_batch_90d[ii,:,:,:]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(R\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, seq8to0\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    162\u001b[0m                                         G\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, seq8to0\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    163\u001b[0m                                         B\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, seq8to0\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kkk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start1,end1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    166\u001b[0m     traindata_batch_45d[ii,:,:,kkk\u001b[38;5;241m-\u001b[39mstart1]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(R\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, (\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m-\u001b[39mkkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    167\u001b[0m                                                        G\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, (\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m-\u001b[39mkkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[1;32m--> 168\u001b[0m                                                        B\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, (\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m-\u001b[39mkkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    170\u001b[0m     traindata_batch_m45d[ii,:,:,kkk\u001b[38;5;241m-\u001b[39mstart1]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(R\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, kkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    171\u001b[0m                                                         G\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, kkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    172\u001b[0m                                                         B\u001b[38;5;241m*\u001b[39mtraindata_all[image_id:image_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, idx_start: idx_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, idy_start: idy_start\u001b[38;5;241m+\u001b[39mscale\u001b[38;5;241m*\u001b[39minput_size:scale, kkk\u001b[38;5;241m+\u001b[39mix_rd, kkk\u001b[38;5;241m+\u001b[39miy_rd,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m traindata_batch_label  <-- scale_factor*traindata_label[random_index, scaled_label_size, scaled_label_size] \u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m '''\u001b[39;00m                \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training start\")\n",
    "for epoch in range(0, 20000):\n",
    "\n",
    "    net.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    train_loss_Epoch = 0\n",
    "    test_loss = 0\n",
    "    loss_division_train = 0\n",
    "    loss_division_test = 0\n",
    "\n",
    "    #===============================#\n",
    "    #           Training            #\n",
    "    #===============================#\n",
    "    print(f\"epoch : {epoch}\")\n",
    "\n",
    "    for nBatch, (x_90d, x_0d, x_45d, x_m45d, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        #print(nBatch)\n",
    "\n",
    "        loss_division_train += 1\n",
    "        train_loss_Batch = 0.0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_0d = x_0d.clone().to(\"cuda\")\n",
    "        x_90d = x_90d.clone().to(\"cuda\")\n",
    "        x_45d = x_45d.clone().to(\"cuda\")\n",
    "        x_m45d = x_m45d.clone().to(\"cuda\")\n",
    "        labels = labels.clone().to(\"cuda\")\n",
    "        #print(f\"{x_0d.max()},{x_0d.min()},{x_90d.max()},{x_90d.min()},{x_45d.max()},{x_45d.min()}\")\n",
    "\n",
    "        outputs = net(x_0d, x_90d, x_45d, x_m45d).squeeze(1)\n",
    "\n",
    "        loss = mae_loss(labels,outputs)\n",
    "\n",
    "        train_loss_Epoch += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #===============================#\n",
    "        #          Validation           #\n",
    "        #===============================#\n",
    "    \n",
    "    for nBatch, (x_90d, x_0d, x_45d, x_m45d, labels) in enumerate(test_dataloader):\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            loss_division_test += 1\n",
    "            train_loss_Batch = 0.0\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_0d = x_0d.clone().to(\"cuda\")\n",
    "            x_90d = x_90d.clone().to(\"cuda\")\n",
    "            x_45d = x_45d.clone().to(\"cuda\")\n",
    "            x_m45d = x_m45d.clone().to(\"cuda\")\n",
    "            labels = labels.clone().to(\"cuda\")\n",
    "\n",
    "            outputs = net(x_0d, x_90d, x_45d, x_m45d).squeeze(1)\n",
    "            loss = mae_loss(labels,outputs)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # validation lossが改善されたときのみ実行\n",
    "    \n",
    "    validatioin_loss = test_loss / loss_division_test\n",
    "\n",
    "    if best_validation_loss > validatioin_loss:\n",
    "        best_validation_loss = validatioin_loss\n",
    "\n",
    "    if epoch % 10 == 0: #10回に1回BPとMSEを計測\n",
    "        print('Validating')\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            x_0d = validation_full_0d.clone().to(\"cuda\")\n",
    "            x_90d = validation_full_90d.clone().to(\"cuda\")\n",
    "            x_45d = validation_full_45d.clone().to(\"cuda\")\n",
    "            x_m45d = validation_full_M45d.clone().to(\"cuda\")\n",
    "\n",
    "            outputs = net(x_0d, x_90d, x_45d, x_m45d)\n",
    "\n",
    "            train_error, train_bp=display_current_output(outputs.detach().cpu().numpy(), \\\n",
    "                                    testdata_label, f\"./validation_output/val_{epoch}.png\",True)\n",
    "\n",
    "            training_mean_squared_error_x100=100*np.average(np.square(train_error))\n",
    "            training_bad_pixel_ratio=100*np.average(train_bp)\n",
    "\n",
    "            #save_validation_tensor_as_png(outputs.squeeze(1),f\"./images/va_e_{epoch}.png\")\n",
    "    \n",
    "        with open(\"./loss/loss_validation.txt\", \"a\") as file:\n",
    "            file.write(f\"Epoch {epoch}/, MSE : {training_mean_squared_error_x100} BP : {training_bad_pixel_ratio}\\n\")\n",
    "\n",
    "        net.train()\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    with open(\"./loss/loss_training.txt\", \"a\") as file:\n",
    "        file.write(f\"Epoch {epoch}/, train Loss: {train_loss_Epoch/loss_division_train}  validation loss : {validatioin_loss}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    if best_BadPixel > training_bad_pixel_ratio:\n",
    "        MSE_record = f\"{training_mean_squared_error_x100:.4f}\".replace(\".\",\"p\")\n",
    "        BP_record = f\"{training_bad_pixel_ratio:.4f}\".replace(\".\",\"p\")\n",
    "\n",
    "        filename = f\"epoch_{epoch}_MSE_{MSE_record}_BP_{BP_record}.pth\"\n",
    "        save_model_and_optimizer(net, optimizer, f\"./model_checkpoint/{filename}\")\n",
    "        best_BadPixel = training_bad_pixel_ratio\n",
    "\n",
    "\n",
    "    if best_BadPixel > training_bad_pixel_ratio:\n",
    "        MSE_record = f\"{training_mean_squared_error_x100:.4f}\".replace(\".\",\"p\")\n",
    "        BP_record = f\"{training_bad_pixel_ratio:.4f}\".replace(\".\",\"p\")\n",
    "\n",
    "        filename = f\"epoch_{epoch}_MSE_{MSE_record}_BP_{BP_record}.pth\"\n",
    "        save_model_and_optimizer(net, optimizer, f\"./model_checkpoint/{filename}\")\n",
    "        best_MSE = training_mean_squared_error_x100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
