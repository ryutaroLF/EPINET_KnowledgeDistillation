{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "from epinet_fun.func_generate_traindata import generate_traindata_for_train\n",
    "from epinet_fun.func_generate_traindata import data_augmentation_for_train\n",
    "from epinet_fun.func_generate_traindata import generate_traindata512\n",
    "from epinet_fun.func_makeinput import make_multiinput\n",
    "from epinet_fun.func_pfm import read_pfm\n",
    "from epinet_fun.func_savedata import display_current_output\n",
    "from epinet_fun.util import load_LFdata\n",
    "\n",
    "from network.model_for_eval_simple import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import threading\n",
    "import configparser\n",
    "import json\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "#from epinet_fun.func_middle_output import middle_layer_output\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inifile = configparser.ConfigParser()\n",
    "inifile.read('./config.ini', 'UTF-8')\n",
    "\n",
    "train_dataset_list = json.loads(inifile.get('dataset_list','train_dataset_list'))\n",
    "test_dataset_list = json.loads(inifile.get('dataset_list','test_dataset_list'))\n",
    "\n",
    "dataset_path = inifile.get('PATH','dataset_path')\n",
    "boolmask_img4_path = inifile.get('PATH','boolmask_img4')\n",
    "boolmask_img6_path = inifile.get('PATH','boolmask_img6')\n",
    "boolmask_img15_path = inifile.get('PATH','boolmask_img15')\n",
    "\n",
    "image_width = int(inifile.get('model_1371','image_width'))\n",
    "image_height = int(inifile.get('model_1371','image_height'))\n",
    "\n",
    "batch_size_training = int(inifile.get('training_general','batch_size_training'))\n",
    "batch_size_validation = int(inifile.get('training_general','batch_size_validation'))\n",
    "batch_num_in_1epoch_for_training = int(inifile.get('training_general','batch_num_in_1epoch_for_training'))\n",
    "training_img_size = int(inifile.get('training_general','training_img_size'))\n",
    "validation_img_size = int(inifile.get('training_general','validation_img_size'))\n",
    "\n",
    "learning_rate = float(inifile.get('training_general','learning_rate'))\n",
    "validation_frequency = int(inifile.get('training_general','validation_frequency'))\n",
    "save_model_frequency = int(inifile.get('training_general','save_model_frequency'))\n",
    "\n",
    "input_ch = int(inifile.get('training_general','input_ch'))\n",
    "filter_num = int(inifile.get('training_general','filter_num'))\n",
    "stream_num = int(inifile.get('training_general','stream_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    normalized_image = (tensor - tensor.min()) / (tensor.max() - tensor.min())\n",
    "    image_uint8 = np.uint8(normalized_image * 255)\n",
    "\n",
    "    concatenated_images = np.hstack(image_uint8)\n",
    "    imageio.imsave(save_path, np.squeeze(concatenated_images))\n",
    "\n",
    "def save_tensor_as_png(tensor,save_path):\n",
    "\n",
    "    tensor = tensor.detach().cpu().numpy()\n",
    "    imageio.imsave(save_path, np.squeeze(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_optimizer(model, optimizer, save_path):\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    state = {\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_current_output(train_output, traindata_label, save_path):\n",
    "    '''\n",
    "        display current results from EPINET\n",
    "        and save results in /current_output\n",
    "    '''\n",
    "\n",
    "    directory_path = os.path.dirname(save_path)\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "\n",
    "    sz=len(traindata_label)\n",
    "    train_output=np.squeeze(train_output)\n",
    "    if(len(traindata_label.shape)>3 and traindata_label.shape[-1]==9): # traindata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15,4,4]\n",
    "    else: # valdata\n",
    "        pad1_half=int(0.5*(np.size(traindata_label,1)-np.size(train_output,1)))\n",
    "        train_label482=traindata_label[:,15:-15,15:-15]\n",
    "\n",
    "    train_output482=train_output[:,15-pad1_half:482+15-pad1_half,15-pad1_half:482+15-pad1_half]\n",
    "\n",
    "    train_diff=np.abs(train_output482-train_label482)\n",
    "    train_bp=(train_diff>=0.07)\n",
    "    condition = train_bp > 0\n",
    "\n",
    "    bp_img=np.zeros_like(train_bp)\n",
    "    bp_img[condition] = 1\n",
    "    bp_img[~condition] = 0\n",
    "    \n",
    "\n",
    "    train_output482_all=np.zeros((3*482,sz*482),np.uint8)\n",
    "    train_output482_all[0:482,:]=np.uint8(25*np.reshape(np.transpose(train_label482,(1,0,2)),(482,sz*482))+100)\n",
    "    train_output482_all[482:2*482,:]=np.uint8(25*np.reshape(np.transpose(train_output482,(1,0,2)),(482,sz*482))+100)\n",
    "    train_output482_all[2*482:3*482,:]=np.uint8(25*np.reshape(np.transpose(bp_img,(1,0,2)),(482,sz*482))+100)\n",
    "\n",
    "    imageio.imsave(save_path, np.squeeze(train_output482_all))\n",
    "\n",
    "    return train_diff, train_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset:\n",
    "    def __init__(self,traindata_all, traindata_label, input_size,label_size,batch_size,Setting02_AngualrViews,\n",
    "                                                boolmask_img4,boolmask_img6,boolmask_img15, batch_num_in_1epoch, mode):\n",
    "        self.traindata_all = traindata_all\n",
    "        self.traindata_label = traindata_label\n",
    "        self.input_size = input_size\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.Setting02_AngualrViews = Setting02_AngualrViews\n",
    "        self.boolmask_img4 = boolmask_img4\n",
    "        self.boolmask_img6 = boolmask_img6\n",
    "        self.boolmask_img15 = boolmask_img15\n",
    "        self.batch_num_in_1epoch = batch_num_in_1epoch\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        (traindata_batch_90d, traindata_batch_0d,traindata_batch_45d, traindata_batch_m45d,\n",
    "        traindata_label_batchNxN)= generate_traindata_for_train(self.traindata_all,self.traindata_label,\n",
    "                                                                self.input_size,self.label_size,1,\n",
    "                                                                self.Setting02_AngualrViews,\n",
    "                                                                self.boolmask_img4,self.boolmask_img6,self.boolmask_img15,self.mode)\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (1, 25, 25, 9) nd.array\n",
    "         traindata_label_batchNxN : (1, 3, 3) nd.array\n",
    "        \"\"\"\n",
    "\n",
    "        (traindata_batch_90d_aug, traindata_batch_0d_aug,traindata_batch_45d_aug,traindata_batch_m45d_aug,\n",
    "        traindata_label_batchNxN_aug) =  data_augmentation_for_train(traindata_batch_90d,traindata_batch_0d,\n",
    "                                                                traindata_batch_45d,traindata_batch_m45d,\n",
    "                                                                traindata_label_batchNxN, 1)\n",
    "\n",
    "        traindata_batch_90d = torch.from_numpy(traindata_batch_90d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_0d = torch.from_numpy(traindata_batch_0d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_45d = torch.from_numpy(traindata_batch_45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_batch_m45d = torch.from_numpy(traindata_batch_m45d_aug).squeeze(0).to(torch.float32).permute(2, 0, 1)\n",
    "        traindata_label_batchNxN = torch.from_numpy(traindata_label_batchNxN_aug).squeeze(0).to(torch.float32)\n",
    "\n",
    "        \"\"\"\n",
    "         traindata_batch_0d : (9, 25, 25) tensor\n",
    "         traindata_label_batchNxN : (3, 3) tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return traindata_batch_90d, traindata_batch_0d, traindata_batch_45d, traindata_batch_m45d, traindata_label_batchNxN\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_size * self.batch_num_in_1epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional/antinous\n",
      "additional/boardgames\n",
      "additional/dishes\n",
      "additional/greek\n",
      "additional/kitchen\n",
      "additional/medieval2\n",
      "additional/museum\n",
      "additional/pens\n",
      "additional/pillows\n",
      "additional/platonic\n",
      "additional/rosemary\n",
      "additional/table\n",
      "additional/tomb\n",
      "additional/tower\n",
      "additional/town\n",
      "additional/vinyl\n",
      "stratified/backgammon\n",
      "stratified/dots\n",
      "stratified/pyramids\n",
      "stratified/stripes\n",
      "training/boxes\n",
      "training/cotton\n",
      "training/dino\n",
      "training/sideboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\\n traindata_label : (16, 512, 512) nd.array\\n testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\\n testdata_label : (8, 512, 512)   nd.array\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata_all,traindata_label=load_LFdata(train_dataset_list)\n",
    "testdata_all,testdata_label=load_LFdata(test_dataset_list)\n",
    "\"\"\"\n",
    " traindata_all  :  (16, 512, 512, 9, 9, 3) nd.array\n",
    " traindata_label : (16, 512, 512) nd.array\n",
    " testdata_all : (8, 512, 512, 9, 9, 3)  nd.array\n",
    " testdata_label : (8, 512, 512)   nd.array\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=23+2         # Input size should be greater than or equal to 23\n",
    "label_size=input_size-22 # Since label_size should be greater than or equal to 1\n",
    "Setting02_AngualrViews = np.array([0,1,2,3,4,5,6,7,8])  # number of views ( 0~8 for 9x9 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make training tensor for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset tensor size : torch.Size([16, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "training_full_90d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_0d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "training_full_M45d = torch.zeros((batch_size_training, training_img_size, training_img_size, 9))\n",
    "\n",
    "for batch, image_path in enumerate(train_dataset_list):\n",
    "    \n",
    "    image_path = os.path.join(dataset_path, image_path)\n",
    "    (train_90d_np, train_0d_np, train_45d_np, train_M45d_np) = make_multiinput(image_path,\n",
    "                                                                              training_img_size,\n",
    "                                                                              training_img_size,\n",
    "                                                                              Setting02_AngualrViews)\n",
    "    train_90d = torch.from_numpy(np.squeeze(train_90d_np))\n",
    "    train_0d = torch.from_numpy(np.squeeze(train_0d_np))\n",
    "    train_45d = torch.from_numpy(np.squeeze(train_45d_np))\n",
    "    train_M45d = torch.from_numpy(np.squeeze(train_M45d_np))\n",
    "\n",
    "    training_full_90d[batch, :, :, :] = train_90d\n",
    "    training_full_0d[batch, :, :, :] = train_0d\n",
    "    training_full_45d[batch, :, :, :] = train_45d\n",
    "    training_full_M45d[batch, :, :, :] = train_M45d\n",
    "\n",
    "training_full_90d = training_full_90d.permute(0, 3, 1, 2)\n",
    "training_full_0d = training_full_0d.permute(0, 3, 1, 2)\n",
    "training_full_45d = training_full_45d.permute(0, 3, 1, 2)\n",
    "training_full_M45d = training_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"training dataset tensor size : {training_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset tensor size : torch.Size([8, 9, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "validation_full_90d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_0d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "validation_full_M45d = torch.zeros((batch_size_validation,validation_img_size,validation_img_size,9))\n",
    "\n",
    "for batch, image_path in enumerate(test_dataset_list):\n",
    "\n",
    "    image_path = os.path.join(dataset_path,image_path)\n",
    "    (val_90d_np , val_0d_np, val_45d_np, val_M45d_np)=make_multiinput(image_path,\n",
    "                                                            validation_img_size,\n",
    "                                                            validation_img_size,\n",
    "                                                            Setting02_AngualrViews)\n",
    "    val_90d = torch.from_numpy(np.squeeze(val_90d_np))\n",
    "    val_0d = torch.from_numpy(np.squeeze(val_0d_np))\n",
    "    val_45d = torch.from_numpy(np.squeeze(val_45d_np))\n",
    "    val_M45d = torch.from_numpy(np.squeeze(val_M45d_np))\n",
    "\n",
    "    validation_full_90d[batch, :, :, :] = val_90d\n",
    "    validation_full_0d[batch, :, :, :] = val_0d\n",
    "    validation_full_45d[batch, :, :, :] = val_45d\n",
    "    validation_full_M45d[batch, :, :, :] = val_M45d\n",
    "\n",
    "validation_full_90d = validation_full_90d.permute(0, 3, 1, 2)\n",
    "validation_full_0d = validation_full_0d.permute(0, 3, 1, 2)\n",
    "validation_full_45d = validation_full_45d.permute(0, 3, 1, 2)\n",
    "validation_full_M45d = validation_full_M45d.permute(0, 3, 1, 2)\n",
    "\n",
    "print(f\"validation dataset tensor size : {validation_full_90d.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup boolmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolmask_img4_path : ../../hci_dataset/additional_invalid_area/kitchen/input_Cam040_invalid_ver2.png\n"
     ]
    }
   ],
   "source": [
    "print(f\"boolmask_img4_path : {boolmask_img4_path}\")\n",
    "boolmask_img4 = np.array(Image.open(boolmask_img4_path))\n",
    "boolmask_img6 = np.array(Image.open(boolmask_img6_path))\n",
    "boolmask_img15 = np.array(Image.open(boolmask_img15_path))\n",
    "\n",
    "boolmask_img4  = 1.0*boolmask_img4[:,:,3]>0\n",
    "boolmask_img6  = 1.0*boolmask_img6[:,:,3]>0\n",
    "boolmask_img15 = 1.0*boolmask_img15[:,:,3]>0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize loss txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(traindata_all, traindata_label, input_size,label_size,batch_size_training,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,batch_num_in_1epoch_for_training,mode=\"training\")\n",
    "test_dataset = CustomDataset(testdata_all, testdata_label, input_size,label_size,batch_size_validation,\n",
    "                              Setting02_AngualrViews,boolmask_img4,boolmask_img6,boolmask_img15,1,mode=\"validation\")\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size_training,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=batch_size_validation,\\\n",
    "                                shuffle=True, num_workers=0, pin_memory=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EPINET(input_ch = input_ch, filter_num = filter_num, stream_num =stream_num).to(\"cpu\")\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "batch = 6\n",
    "\n",
    "x_0d = validation_full_0d[batch,:,:,:].clone().to(\"cpu\").unsqueeze(0)\n",
    "x_90d = validation_full_90d[batch,:,:,:].clone().to(\"cpu\").unsqueeze(0)\n",
    "x_45d = validation_full_45d[batch,:,:,:].clone().to(\"cpu\").unsqueeze(0)\n",
    "x_m45d = validation_full_M45d[batch,:,:,:].clone().to(\"cpu\").unsqueeze(0)\n",
    "\n",
    "x_0d_1,x_0d_2,x_0d_3,x,x_1,x_2,x_3,x_4,x_5,x_6,x_7 = net(x_0d, x_90d, x_45d, x_m45d,batch)\n",
    "tensors = [x_0d_1, x_0d_2, x_0d_3, x, x_1, x_2, x_3, x_4, x_5, x_6]\n",
    "tensors_name = [\"x_0d_1\", \"x_0d_2\", \"x_0d_3\", \"x\", \"x_1\", \"x_2\", \"x_3\", \"x_4\", \"x_5\", \"x_6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tensor in enumerate(tensors):\n",
    "    sliced_tensor = tensor[0, 2, :, :].detach().numpy()\n",
    "\n",
    "    # Plot the sliced tensor\n",
    "    plt.imshow(sliced_tensor, cmap='gray')\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "\n",
    "    # Save the figure\n",
    "    filename = f'./hidden_img/hidden_output_{tensors_name[i]}.png'\n",
    "    plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()  # Close the plot to avoid displaying it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Tensor names, assuming they are the same as in the original code snippet\n",
    "tensors_name = ['x_0d_1', 'x_0d_2', 'x_0d_3', 'x', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6']\n",
    "\n",
    "# Read images from the specified file paths\n",
    "images = []\n",
    "for name in tensors_name:\n",
    "    filepath = f'./hidden_img/hidden_output_{name}.png'\n",
    "    if os.path.exists(filepath):\n",
    "        images.append(Image.open(filepath))\n",
    "\n",
    "# Check if there are any images loaded\n",
    "if not images:\n",
    "    print(\"No images found in the specified directory.\")\n",
    "else:\n",
    "    # Determine the grid size for the tile plot\n",
    "    num_images = len(images)\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "    # Create a blank canvas to place the images\n",
    "    img_width, img_height = images[0].size\n",
    "    canvas = Image.new('RGB', (grid_size * img_width, grid_size * img_height))\n",
    "\n",
    "    # Place each image onto the canvas\n",
    "    for i, img in enumerate(images):\n",
    "        row = i // grid_size\n",
    "        col = i % grid_size\n",
    "        canvas.paste(img, (col * img_width, row * img_height))\n",
    "\n",
    "    # Display the canvas\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Save the figure\n",
    "    tile_image_path = './hidden_img/tiled_hidden_outputs.png'\n",
    "    canvas.save(tile_image_path)\n",
    "    plt.close()  # Close the plot to avoid displaying it again\n",
    "\n",
    "    tile_image_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in range(70):\n",
    "    for i, tensor in enumerate(tensors):\n",
    "        sliced_tensor = tensor[0, batch, :, :].detach().numpy()\n",
    "\n",
    "        # Plot the sliced tensor\n",
    "        plt.imshow(sliced_tensor, cmap='gray')\n",
    "        plt.axis('off')  # Turn off the axis\n",
    "\n",
    "        # Save the figure\n",
    "        filename = f'./hidden_img/hidden_output_{tensors_name[i]}.png'\n",
    "        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()  # Close the plot to avoid displaying it again\n",
    "\n",
    "\n",
    "    images = []\n",
    "    for name in tensors_name:\n",
    "        filepath = f'./hidden_img/hidden_output_{name}.png'\n",
    "        if os.path.exists(filepath):\n",
    "            images.append(Image.open(filepath))\n",
    "\n",
    "    # Check if there are any images loaded\n",
    "    if not images:\n",
    "        print(\"No images found in the specified directory.\")\n",
    "    else:\n",
    "        # Determine the grid size for the tile plot\n",
    "        num_images = len(images)\n",
    "        grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "        # Create a blank canvas to place the images\n",
    "        img_width, img_height = images[0].size\n",
    "        canvas = Image.new('RGB', (grid_size * img_width, grid_size * img_height))\n",
    "\n",
    "        # Place each image onto the canvas\n",
    "        for i, img in enumerate(images):\n",
    "            row = i // grid_size\n",
    "            col = i % grid_size\n",
    "            canvas.paste(img, (col * img_width, row * img_height))\n",
    "\n",
    "        # Display the canvas\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Save the figure\n",
    "        tile_image_path = f'./hidden_img/_batch{batch}.png'\n",
    "        canvas.save(tile_image_path)\n",
    "        plt.close()  # Close the plot to avoid displaying it again\n",
    "\n",
    "        tile_image_path\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in range(70):\n",
    "\n",
    "    image_paths = [f'./hidden_img/_batch{batch}.png' for batch in range(70)]\n",
    "    frames = [Image.open(image_path) for image_path in image_paths if os.path.exists(image_path)]\n",
    "\n",
    "    # アニメーションGIFを作成する（画像が存在する場合のみ）\n",
    "    if frames:\n",
    "        gif_path = './hidden_img/hidden_output_animation.gif'\n",
    "        frames[0].save(gif_path, format='GIF', append_images=frames[1:], save_all=True, duration=100, loop=0)\n",
    "    else:\n",
    "        print(\"No images found to create an animation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
